%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Copyright (c) 2011, ETH Zurich.
% All rights reserved.
%
% This file is distributed under the terms in the attached LICENSE file.
% If you do not find this file, copies can be found by writing to:
% ETH Zurich D-INFK, Haldeneggsteig 4, CH-8092 Zurich. Attn: Systems Group.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[a4paper,twoside]{report} % for a report (default)

\usepackage{bftn} % You need this

\title{Bulk Transfer}   % title of report
\author{Pravin Shinde} % author
\tnnumber{014}  % give the number of the tech report
\tnkey{Bulk Transfer: The mechanism to transfer large data} 
% Short title, will appear in footer

% \date{Month Year} % Not needed - will be taken from version history

\begin{document}
\maketitle

%
% Include version history first
%
\begin{versionhistory}
\vhEntry{1.0}{11.08.2011}{TR}{Initial version}
\end{versionhistory}

% \intro{Abstract}      % Insert abstract here
% \intro{Acknowledgements}  % Uncomment (if needed) for acknowledgements
% \tableofcontents        % Uncomment (if needed) for final draft
% \listoffigures        % Uncomment (if needed) for final draft
% \listoftables         % Uncomment (if needed) for final draft

\chapter{Introduction}
This technical note is aimed to document the design of ideal cross domain bulk
transfer mechanism that we wish to have in Barrelfish.


\section{Existing implementation in Barrelfish}
There are two bulk transfer mechanisms that exist in the current(as of August
2011) implementation of Barrelfish.
\texttt{$lib/barrelfish/bulk\_transfer.c$}
and \textit{pbufs} used in the network stack.  We will discuss them briefly in
following two sub-sections.

\subsection{$bulk\_transfer.c/h$}
This is the formal bulk transfer method supported by Barrelfish.  In brief, this 
facility works by sharing a continuous piece of memory (in form of capability) 
between two domains.  These domains then map this physical memory into their 
virtual address-space.  The virtual address where this shared memory is loaded
can be different.  So, participating processes can't exchange the virtual 
addresses directly.  Bulk transfer mechanism works by dividing the entire 
memory area into the blocks of fixed size.  The domain which initiated the 
bulk transfer is responsible for managing these blocks.  This responsibility
includes following.
\begin{enumerate}
  \item Allocation and deallocation of the blocks.
  \item Maintaining the information about free and used blocks.
\end{enumerate}
So, this bulk transfer facility works in master-slave setup.  The master
allocates the blocks and passes the index of the block to slave.  
Once the block is passed to the slave, master should not touch it.  
The slave can locate the block by adding the \texttt{(index*block-size)} to the
virtual address pointing to the beginning of the shared memory area.
Now slave can read/modify the contents in the memory area of this block.  Once
the slave is done with accessing the block, it can pass the block-index
back to the master.  Master can either access the block or release it,
so that it will be added to the list of free blocks.

\subsubsection{Limitations}
\paragraph{Security}
This transfer mechanism can only be used between domains which are
co-operative and willing to follow the protocol set for using the bulk
transfer mechanism.  Malicious domain can corrupt the data in shared memory
or can refuse to return the blocks once passed to it.

\paragraph{More than two domains}
The design of this bulk transport does not stop you from using it with
more than two domains as long as the protocol is followed.  But current
implementation does not track which domain is holding the buffer. So,
one can use the current implementation with multiple domains as long
as the domains are co-operative and applications are willing to deal
with added complexity of tracking which domains hold which buffers.

\paragraph{Use}
Due to the relatively simple nature of the implementation, its use is
limited only to few places. And hence it is not throughly tested in
various scenarios.  

\subsection{pbufs}
The network stack uses its own custom bulk transport mechanisms.  Lets
call them pbufs.  These pbufs work in similar way to above mechanism,
but are more flexible.  The application shares some piece of physical memory
with network driver which is used as shared memory.  Then application
creates a list of pbuf structures, each one of them holding an offset
within shared memory, length, \textit{pbuf-id} and the shared memory id to
which they belong.  The key difference here is that the these pbufs 
may not hold consecutive memory locations even when \textit{pbuf-id}'s are
consecutive.  In contrast with Barrelfish bulk-transfer where buffer-id
value is enough to find the location of buffer within shared memory,
pbuf needs to store the offset separately in another list.  So,
pbufs just provide another layer of indirection to allow more
flexible use of memory.


The way this mechanism currently works is that, application creates
pool of initial pbufs and registers them with network driver.  Both,
application and network driver maintain the list of pbufs in their
private memory.  This list is kept in synchronization by sending
explicit messages.  Now each pbuf can in principle point to any
buffer of any size, located anywhere in the shared memory.


This flexibility is used by application when it receives a data from
the network driver.  Application creates a new pbuf structure with
same pbuf-id but pointing at new buffer location and send it back to
the driver as new free pbuf to use.  And the location of previous
buffer is used for processing the data.  This way, application can
return the pbufs back to driver ASAP without getting affected by how
long does the data processing takes.  When application is done with
processing the data in that buffer, it releases that buffer.  This
released buffer is then used to create new pbuf which will be
registered again with driver in future. 

\subsubsection{Limitations}

\paragraph{Needs more memory}
The shared memory needs to be bigger than the memory shared with
network driver in form of registered pbufs.  This is because, at any
given point in time, some pbufs will be in application data processing
phase and hence can-not be used by driver to receive the new data.

\paragraph{Data corruption}
This bulk-transfer mechanism assumes co-operative domains.  Both
domains have read/write access to the shared memory at all the time.
This means that if they do not follow the protocol correctly, they may
end up writing at same physical location, leading to data corruption.

\paragraph{Complicated memory reclamation}
The current implementation of pbufs assumes that application can have
multiple threads and all of them can access the data in pbufs which
are delivered to the application.  This complicates the problem of
detecting when exactly all threads are done with accessing the
particular pbuf.  The current implementation uses a reference count
mechanism for this detection.  Even though this memory reclamation is
functional, it is complicated and one can easily get it wrong, leading
to memory leaks.

\paragraph{Supporting more than two domains}
In theory, this design can be used with more than two domains which
are co-operating with each other, but implementation is not designed
with such a case in mind.  The problematic issues will be tracking
which memory buffer is with which domain, and when the memory buffer
can be reused.

\subsubsection{More issues with current network stack implementation}
Following issues are not exactly due to the bulk transfer mechanisms,
but are mainly the implementation issues of network stack.  I am
documenting them here for sake of completeness.  Also, knowing the
issues in current stack will help in understanding the design
decisions made.

\paragraph{Favors the setup with driver on different core}
It is optimized for the case where application and network driver are
running on separate core.  It is reasonable to prefer such scenarios
for multicore architectures where presence of large number of cores is
assumed.  This preference has resulted on overly dependence on
efficiency of UMP messages.  Current implementation sends around 2
messages (send-packet, tx-done) for transmitting single packet
assuming that packet fits into one pbuf.  Similarly on packet
receiving side, each received packet involves 2 messages 
(register-pbuf, packet-received).  The performance impact of these
messages is not strongly visible when application and driver are on
different core.  But there are cases like SCC where it is good idea 
to run the driver on same core as an application.  In such scenarios
LMP's are used for communication which involves doing context-switch in
sending every message. This leads to lot of performance
in-efficiencies when current network stack implementation is used in
LMP contexts.  Specially on those hardware where cost of
context-switch is relatively high.


I have tried to measure the performance difference on \texttt{nos4-6}
boxes with webserver cache loading test, but the performance
difference was not significant.  I have not investigated the reasons
behind them yet, but my preliminary guess was that context switches on
these machines are quite efficient or webserver cache loading test
does not stress the network stack enough to see any significant
difference.


\section{Questions to be asked about bulk transfer mechanisms}
\begin{enumerate}
  \item Does it uses memory efficiently?
  \item Can it be used with mutually non-trusting domains?
  \item Can it be used across more than two domains?
  \item Can consumer/producer put a back-pressure on other side?
  \item Can it support multiple producer and multiple consumers?
  \item Can it allow implementation of \textit{True Zero Copy}
  sending and receiving?
  \item Can it lead to deadlock if one domain is only sending without
  checking for arriving messages?
\end{enumerate}

\section{Desired properties from bulk transfer mechanisms}

\section{Related work}
The issues of bulk-transport has been investigated quite a few times
in the past, so the solution of this problem is known to the large
extent.  What this document is aiming is to choose right approach and
right solution which will suit the needs of the Barrelfish.


The work which has particularly influenced this design are
\begin{enumerate}
  \item F-bufs: Helped in clearly understanding the problems in
  designing cross-domain transfer facility.
  \item R-bufs: Helped in understanding how to implement
  scatter-gather packet sending and receiving.
  \item Beltway buffers: Gave clear idea of how to design a
  system with multiple ring-buffer to get lot of flexibility.
  The problem with beltway buffers is that they are
  designed to be POSIX compatible and due to that, the whole
  functionality of these buffers is pushed into trusted OS code-base.
  The applications are unaware of these buffers.  Also, these buffers
  can be used mostly to short-circuit the data path between different
  trusted kernel sub-systems.  If applications are going to modify the
  data then beltway buffers don't help much.
\end{enumerate}

\section{Requirements of Barrelfish}

\begin{enumerate}
  \item Avoid data copy as much as possible, if can't avoid,
  push it into user-space/user-core.
  \item Ability to batch the notifications.
  \item True zero copy capability(scatter-gather packet
  sending/receiving).
  \item Should work with more than two domains.
  \item Should work with multiple producers and multiple consumers.
\end{enumerate}

\chapter{Design}
This chapter discusses the design of bulk-transport mechanism and
protocol that will be implemented in future for Barrelfish network
stack.


\section{Bird's eye view of design}
We are aiming to design a cross domain bulk transport mechanism,
which supports following features.
\begin{enumerate}
  \item Should reduce the data copy as much as possible.
  \item Should exploit the fact that complete isolation of data
  is not always needed.
  \item More than two separate domains should be able to share
  the data without copying it.
  \item Number of explicit notifications needed should be low.
  \item It should work in \textit{single producer, single consumer}
  and \textit{single producer, multiple consumers}
\end{enumerate}

\section{Terminology used}
This section briefly explains the term used in the design 
description.  This should help in the understanding by reducing 
the ambiguity of the terms.
\begin{enumerate}
  \item \textbf{Generator:} An entity which is generating the data.  It
  can be software or hardware entity.  For example, Network
  Interface Card(NIC) is a generator, or the application code
  which is generating the packets to send over the NIC.
  \item \textbf{data-element:} The data-element is what generator 
  is generating. In case of NIC device, a packet will be a
  data-element.
  \item \textbf{Producer:} An entity which is managing the generated data.
  This management contains providing access mitigation, notifying
  interested consumers, reclaiming the memory when data-element
  expires.  The example of producer will be \textit{NIC device
  driver}.  


  The distinction between generator and producer is
  rather fine.  They are intentionally kept separate because
  generators can have hardware constraints which does not allow
  it to do all functionalities.
  \item \textbf{Consumer:} An entity which is consuming the data-elements. 
  \item \textbf{Slot:} The contiguous piece of memory where data-element is
  entirely or partially stored. A data-element can span over
  more than one slots. 
  \item \textbf{Shared-pool:} A contiguous piece of memory accessible 
   from producer in read/write mode and consumers in read only mode.
   This shared pool is divided into above slots.
  \item \textbf{Production-pool:} It is a collection of all
  shared-pools used by the particular producer.  The initial
  shared-pool is added by the producer, and later on every consumer
  which joins the system contributes a new shared-pool to the
  production-pool.
  \item \textbf{Free slot:} A slot which is available, and can be given
  to generator.
  \item \textbf{In-generation slot:} A slot is given to generator. Only 
  the generator should read/write this slot now.  This is the only
  mode when data can be written in the slot.
  \item \textbf{In-consumption slot:} One or more consumers are currently
  consuming this slot.  In this mode, slot is strictly read-only
  no one should modify the data in slot in this mode.
  \item \textbf{slot-pointer:} A data-structure which holds enough
  information to locate particular slot present in any shared-pool
  with producer.  These are typically used in cross domain
  communication to identify the exact slot.  A typical slot-pointer
  should have a shared-pool-id and offset within shared-pool to
  find the particular slot.
\end{enumerate}

\section{The Producer}
This section describes the producer and generator together with
their responsibilities.  Even though producer and generator can
be a different entities internally, the consumers will only see
the producer interface.

\subsection{Generator}
Lets make a rough sketch of typical generator.  Here we are aiming
for the NIC devices.  Typically, these devices will have an
internal queue (RX-queue) of slots where the packet received
from wire will be copied.  These generators are also capable
of generating a notification in form of interrupt.  Now the
way RX-queues work on different devices differ in some way.
Few devices expect contiguous memory in the slots whereas
other devices are capable of DMAing the packet in non-contiguous
memory as well.

	
The software generators like application
logic which creates new packet to send out is much simpler than
NIC devices but can have quirky behavior like above described
NIC devices.


\subsection{The Producer abstraction}
The producer abstraction is what consumer sees and interacts with.
It is responsible for managing the data produced by generator and
using shared-pools to get that data till consumers.  It is also
responsible for managing the shared-pool memory and co-ordinating 
the access to this shared resource.


\subsection{Shared-pool}
Shared-pool is the area where producer will generate the data
and consumers will read it from. The producer breaks this
shared-pool into \textit{slots}.  The size of slot
is based on the capability of generator.  If generator needs
continuous memory to produce data-element then the size of
slot will be the size of biggest data-element.  If the
generator can DMA the data-element into multiple non-contiguous
memory locations then, the slot size should be based on
the average data-element size.  In any case, the slot size
must alway be a multiple of cache size and every slot should
be cache aligned.  This is mostly due to the fact that each
data element might get consumed by different consumer running
on different cores, and we do not want cache conflicts when
they are accessing different data-elements.


The dis-advantage of having slots of size of largest data-element
is that it leads to internal fragmentation and waste of memory 
when packets are small.  On other hand, when slot size is small,
we reduce the capacity of NIC hardware as typically RX-queue
will have limited number of entries, and by using small sized
slots, we will fill up the entries quickly.


The producer creates an initial \textbf{shared-pool} area.  As new
consumers join the system, each one of them will contribute an
additional piece of shared-pool.


The producer maintains the list of shared-pool and for every
shared-pool, it also maintains the list of consumers who have access
to it.  

\paragraph{Releasing the shared-pool}
If any consumer decides to leave the system, then producer
stops using the shared-pool provided by him.  The producer will 
wait till all the slots in that shared-pool are free and then it will
ask all consumers who have mapped this shared-pool to release the
pool.  Once all consumers release the pool then producer can also
release the pool and inform the consumer who contributed the
shared-pool about completion of the process.

This is rather long process and depends on all the consumers for it's
completion.  This process is not part of the critical path, it is part
of tear-down part so some inefficiency can be tolerated here.


The virtual address where the shared-pool is mapped need not be same for 
the producer and consumers.


\paragraph{Ideal size of production-pool}
The production-pool is just a collection of all valid shared-pools
available at given time.  Ideally, production-pool should be big enough 
to accommodate the queue of the generator (ie. All the descriptors 
in RX-queue of NIC device) and all the data-elements which are with 
consumers and are not released back (in-consumption state).  
So, the total memory size should be proportional to the capacity of 
the generator(ie. NIC device) and number of consumers.  
So, one way to look at this is that, producer will provide shared-pool
which can satisfy the needs of generator and each consumer will
contribute a shared-pool to allow in-consumption packets.
As each consumer is allowed to keep only fixed amount of 
in-consumption packets, slow consumers will not adversely 
affect the fast consumers.


\subsubsection{A meta-slot structure}
This structure hold the additional information about each slot in
a shared-pool.  It can be seen as index table on shared-pool.
This structure is private to Producer and is used to manage the
slots within shared-pool.  Following are the key elements of 
this structure.
\begin{enumerate} 
  \item \textbf{slot-id:} The slot identifier. 
  \item \textbf{offset:} The location of slot within the shared pool.
  \item \textbf{data-len:} Size of valid data in the slot.
  \item \textbf{state:} The state of slot (Free, in-Generation, 
  in-Consumption)
  \item \textbf{consumer-list:} Which consumers are accessing it? 
  \item \textbf{Next, prev:} Used to maintain the list (eg. Free slots)
\end{enumerate} 

\paragraph{Producer initialization}
At producer initialization, following steps will happen.
\begin{enumerate} 
  \item Create and initialize the list of shared-pools and meta-slots.
  \item Add the initial shared-pool big enough to satisfy generator
  needs.  
  \item Divide the shared-pool into slots, create a meta-slots list
  for this shared-pool and add it to meta-slots list.
  \item Create an empty list of consumers.
  \item Initialize the generator (if needed).
\end{enumerate} 

\paragraph{Possible events received by producer}
\begin{enumerate} 
  \item \textbf{New consumer registered:}  This event shows the
  successful completion of \textbf{consumer registration}.  The
  consumer registration is multi-step process and broadly it involves
  establishing communication/notification channel,
  registering a new shared-pool from consumer,
  giving read-only access to existing shared-pools to consumer,
  creating a consumer-queue with consumer.  
  The remaining document will give more details on each of the above
  steps.

  \item \textbf{Consumer de-register:} This event is request by
  consumer to de-register itself.  The producer should not send any
  more notifications to this consumer about new packet.  The producer
  should also start the process or releasing the shared-pool which was
  contributed by this consumer.

  \item \textbf{Consumer-queue space available:} When this
  notification is received from any consumer, that shows that the
  consumer now has free space to receive more data-elements and
  producer should resume sending data-element notifications to it.
  Note here that producer will stop sending the data-notifications as
  soon as the consumer-queue is full for that consumer. Producer will
  not check the status of this consumer-queue until and unless it
  receives this notification.  Consumer will also send this message in
  the beginning when it is done with all the setup and ready to send
  the message.  In simple sense, this message can be seen as telling
  the producer that you are ready to receive data-elements.

  \item Increment data-element counter.

  \item Consumer registers up-call for new data.
  \item Data-element generated (from generator).
  \item generator queue full (from generator).
\end{enumerate} 

\paragraph{Possible events generated by producer}
\begin{enumerate} 
  \item add empty slot (to generator). 
  \item New data element.
  \item Consumer queue full. 
\end{enumerate} 

\section{Consumer}
This section describes the internals of the consumer.

\subsection{Consumer registration}
As part of initialization process, consumer should register itself
with producer.  This initialization process includes following steps.
\begin{enumerate} 
  \item Map the shared-pool as read-only in the virtual
  address space.
  \item Create a new contiguous memory block to maintain a consumer-queue.
  \item Initialize this memory block with consumer-queue
  data-structure. 
  \item Share this memory block with producer.
\end{enumerate} 


\subsection{Consumer-queue}
This section describes the consumer-queue in details.
The consumer-queue is a shared data-structure between consumer and
producer and it is exclusive between each pair of consumer-producer.
This data-structure is also maintained on contiguous shared memory 
which is read/writable by both consumer and producer.  This
data-structure is shared ring-buffer with few virtual-registers 
which are used to manage the access the ring-buffer.

\begin{enumerate} 
  \item \textbf{write-register:} This virtual register is the first
  element in the consumer-queue.  This element should be of size
  cache-line to avoid any cache-conflicts.  This element can be
  modified only by the producer and consumer can only read this value.
  The value in this virtual-register contains the write-index for
  consumer-queue.  \textit{The write-index points to the slot-pointer within
  consumer-queue that producer will produce next.}

  \item \textbf{read-register:} This virtual register is the second
  element in the consumer-queue.  This element should be of size
  cache-line to avoid any cache-conflicts.  This element can be
  modified only by the consumer and producer can only read it.
  The value in this virtual-register contains the read-index for
  consumer-queue.  \textit{The read-index points to the slot-pointer within
  consumer-queue that consumer will consume next.}

  \item \textbf{queue-size-register:} This virtual register is the third
  element in the consumer-queue.  This element can be modified by
  producer and consumer can only read it.  The value in this
  virtual-register indicate the number of slot-pointers present in the
  consumer-queue.  This register allows dynamic adjustment queue-size
  based on the current load.  Similar feature is implemented in the
  \textit{beltway buffers} with the claim that keeping the size of 
  queue as small as possible helps in cache friendliness.  This
  feature is actually optional and will be added in later stages.
  To implement this feature, one also need to made decision like when
  the queue size should be reduced and when it should be increased
  again.  Producer can make these decisions based on how much
  preference it wants to give to this particular consumer over others.

  \item \textbf{Slot-pointers:} After above three registers, the rest
  of the space in consumer-queue is used for storing slot-pointers.
  The slot pointers are used to point a particular slot in one of the
  shared-pools within production-pool of the producer.  
  Slot-pointers have following information.
  \begin{enumerate}
    \item \texttt{shared-pool-id:} Id of the shared-pool which is
    holding this particular slot.  These id's are given by the
    producer and will be unique within that particular for given
    shared-pool.  As the shared-pools within particular
    producer may increase/decrease over time, the consumer may receive
    a slot-pointer with shared-pool-id which it has not mapped yet.
    In such a case, it should send a message to producer asking for
    read-only access to this new shared-pool and then map it into
    the virtual address-space.  Once the shared memory frame
    associated with shared-pool memory is mapped, consumer can 
    continue to access the slot using slot-pointer.
    \item \texttt{slot-index:}  The index of slot within that
    shared-buffer.  This value is only useful for producer as this
    slot-index maps into the meta-slot structure which is private to
    the producer.  Consumer should not alter this value.  In case
    malicious consumer alters this value, the producer will be able
    to detect it as meta-slot structure maintains the information
    about which all consumers are currently consuming the slot and
    whenever the producer reclaims the slots freed by consumer, it
    validates if this consumer is in the list maintained in meta-slot.
    The reason for maintaining this information in slot-index even
    when it is not useful to consumer is because it speeds up the
    producer in relocating the slot within shared-pool when it is
    freed by the consumer.  
    \item \texttt{Offset:} The start of the slot within shared-buffer.
    In case of fixed size slot, offset can be calculated by
    slot-index.
    \item \texttt{More:} As the data-element can span more than one
    slots, this flag tells us if there are more slot-pointers 
    following which belongs to same data-element.  This flag is
    equivalent to the \textbf{More Fragments} bit in the \textbf{IP
    protocol}.  In contrast to IP protocol there is no fragment
    identification number, but the slot-pointers belonging to same
    data-elements are assumed to follow each other.  So, the order in
    which the slot-pointers are added to the consumer-queue is
    important.  As we have only one producer which is adding the
    slot-pointers in \textit{available to consume} section and also
    this producer is dealing with one data-element at one time,
    maintaining this order is possible.
  \end{enumerate} 

\end{enumerate} 


\subsubsection{Conditions on consumer-queue registers}

\begin{enumerate} 
  \item Queue empty condition: read-index == write-index
  \item Queue full condition: ((write-index + 1) \% size) == read-index 
  \item Elements available to consume \newline
  (assuming queue not empty):\newline
  if (write-index $>$ read-index) \newline
    then \{[read-index, (write-index - 1)]\} \newline
    else \{[read-index, (size - 1)], [0, (write-index - 1)]\}  \newline
  \item Elements which are already consumed and are now free.  \newline
  (assuming queue not empty):  \newline
  if (write-index $>$ read-index) \newline
    then \{[write-index, (size - 1)], [0, (read-index - 1)]\} \newline
    else \{[write-index, (read-index - 1)]\} \newline
\end{enumerate} 

\paragraph{Possible events received by consumer}
This paragraph describes the event received by consumers and how
to react to them.

\begin{enumerate} 
  \item \textbf{More data arrived:} This callback is triggered
  whenever the queue was empty and new data arrived after queue being
  empty.  This callback is not triggered for every arrival of data,
  but only when consumer is not explicitly polling for data.  Whenever
  new data arrives on the empty queue, producer can assume that
  consumer is not polling the channel as queue is empty, and hence
  producer should send this notification to consumer to wake it up.
  In other case where queue is non-empty, consumer is already aware of
  the presence of data-elements there.  Consumers are expected to
  deal with growing dynamically growing of queue-size and hence adding
  more elements to non-empty queue without sending explicit
  notifications should not break the consumer-logic.

  \item \textbf{Consumer-queue full:}  This callback is triggered when
  the consumer-queue is full and producer is not able to add new
  data-elements to the queue.  Triggering of this callback means that
  consumer is slow in consuming the data from the consumer-queue. And
  result of this producer is going to drop the data-elements which
  were aimed for this consumer till there is more free space with this
  consumer.  
 
  \item \textbf{Consumer-queue almost full:} This is an optional
  event and is designed as refinement of the basic approach.
  This additional event can be 
  called when the number of free slot-pointers in the queue drops bellow
  certain threshold.  This notification can be used as a warning sign
  to the consumer that either it takes some action to free up more
  slot-pointers in consumer-queue, or producer will soon start dropping the
  data-elements designated to this consumer.

  \item \textbf{Error event:}  This callback is called when something
  unexpected goes wrong.  The information and severity of the error
  will inform the consumer that if the error is transient or
  permanent.  And based on the error type, consumer can take
  corrective actions.  Few samples of these errors are corruption of
  consumer-queue, fetal error in producer, etc.

  \item \textbf{Add shared-pool:}  This is an optional message which
  allow producer to push the notifications about newly added
  shared-pools.  This message is optional because consumers
  can lazily ask for these new shared-pools whenever they 
  encounter them as part of slot-pointer while consuming
  data-elements.  This notification is not considered as part of
  critical path or part of the data-flow.  This notification will be
  generated only on the arrival of new consumer or if producer decides
  to increase the available shared-pools.  This can be classified as
  setup or maintenance path.

 \item \textbf{Remove shared-pool:}  This call is opposite of above
 \textit{add shared-pool} call.  It tells consumer is that producer
 will not be using a particular shared-pool from now onwards
 for whatever reason (eg. the consumer which gave that shared-pool
 frame has terminated the connection with producer).  So, every
 consumer which receives this message should remove the mapping for
 shared-pool from it's virtual memory.  This notification is
 also relatively rare and will be generated only when one of the
 consumer decides to quit or if producer decides to reduce the number
 of available shared-pool.  So, this notification should not
 be considered as part of critical-path or data-flow.  It can be
 classified into setup or maintenance path.
\end{enumerate} 

\paragraph{Possible events generated by consumer}
This paragraph describes the events generated by consumers.

\begin{enumerate} 
  \item \textbf{Consumer-queue space available:} This notification is
  sent to the producer when queue was full and a free slot-pointer is
  added.  Once the queue is full, producer is not expected to check
  the queue status until it receives this message saying that now
  application has more space to receive packet.  As an optimization,
  consumer might wait till some more free-space is accumulated before
  informing the producer to re-start the flow.  This way the
  inefficiency similar to \textit{silly window behavior} in TCP 
  flow control can be avoided.

  \item \textbf{Get frame for shared-pool-id:} This notification is
  sent to the producer when consumer receives a slot-pointer with 
  shared-pool-id which is not mapped by this consumer yet.  By
  sending this notification, consumer is asking for the read-only
  access to this shared-pool.  The producer should respond to
  this message with either with valid capability or with error.

  \item \textbf{Forwarding slot-pointer to other consumer:}  This
  message is related to the ability of consumers to forward
  slot-pointers within each other.  There are two cases here.  In
  first case, the consumer forwards the slot-pointer to other 
  consumer and then also collects it back before declaring it as free.
  In this case, producer does not need to be informed at-all about
  this sharing as consumer is taking responsibility of properly
  freeing the slot.  But in case, consumer just wants to forward the
  slot-pointer and continue on it's own working without bothering
  about when and how other consumer is going to stop the access to
  that slot, then it should inform the producer that it has forwarded
  that slot to some other consumer.  This can be done with this
  message.  The reason we need this separate message is that, 
  because of sharing there will be two consumers which will be 
  releasing the slots and producer should be aware of them.  
  The information from this message is used to update the 
  meta-slot structure for this slot with one more consumer.
  This message contains the slot identification information
  and consumer identification information.  The other consumer can 
  now release the slot whenever it is done using it's consumer-queue.  


  This forwarding has some more implications which are not fully
  explored in this document.  Here are few leads on that.  Firstly,
  as this path allows consumer to receive the slots without being
  initiated by producer, this slot will not be a part of
  consumer-queue.  So, the producer need to somehow account for these
  slots so that it can maintain the fairness in the number of
  outstanding slots per consumer.  This mechanism also needs some way
  in which two consumers can talk with each other and exchange the 
  information like consumer-id's, producer-id and slot-pointers.
  Also, consumers should be careful in dealing this information as
  consumer-id and slot-pointers are valid in context of particular
  producer.



\end{enumerate} 

\subsubsection{Workflow}
The typical working of consumer-queue will be as follows.  The queue
will be initially created empty.  The producer will keep adding the
slot-pointers pointing to the data-element into the queue and
increment the write-index value.  It will also be generating the 
notifications whenever necessary.  Producer will stop
adding elements in the queue when it is full, and resume adding the
elements only when it is non-full again.  The consumer should be 
awaken by the queue non-empty event and it should start consuming 
the data-elements as long as queue has any element.  

\subsubsection{Freeing up the slots}
Whenever consumer is done with accessing/consuming the data-element,
it should be added back to the free-slot-pointers area of the consumer 
queue.  It is important to note that the order in which slots will be
freed by consumer need not match the order in which it consumed the
slots.  This flexibility allows consumer to take longer time on
certain slots while quickly returning the slots which have arrived
afterwards.  Consumer can hold back certain number of slots without
releasing them, but this number depends on the length of the consumer
queue.  As the producer controls the consumer-queue length it can give
some consumers more slots than other based on the configurable
policies.  The policy that initial implementation of this buffer will
be using is that, the size of consumer-queue will be directly
proportional to the size of shared-pool that particular consumer has
contributed.


\section{Notification mechanism}
Describe the notification mechanism.

\section{Memory management mechanism}
Describe the memory management mechanism

\section{How sharing between multiple consumer will happen?}
Details of how things will be managed when there are more than two
applications. Can more than two applications work in sync with true 
zero copy?

\section{The communication between consumers}
TBD:  The communication between consumers is needed to share the
slot-pointers between them.
\end{document}
