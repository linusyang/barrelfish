%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Copyright (c) 2011, ETH Zurich.
% All rights reserved.
%
% This file is distributed under the terms in the attached LICENSE file.
% If you do not find this file, copies can be found by writing to:
% ETH Zurich D-INFK, Haldeneggsteig 4, CH-8092 Zurich. Attn: Systems Group.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[a4paper,twoside]{report} % for a report (default)

\usepackage{bftn} % You need this

\title{Bulk Transfer}   % title of report
\author{Pravin Shinde} % author
\tnnumber{014}  % give the number of the tech report
\tnkey{Bulk Transfer: The mechanism to transfer large data} 
% Short title, will appear in footer

% \date{Month Year} % Not needed - will be taken from version history

\begin{document}
\maketitle

%
% Include version history first
%
\begin{versionhistory}
\vhEntry{1.0}{11.08.2011}{TR}{Initial version}
\end{versionhistory}

% \intro{Abstract}      % Insert abstract here
% \intro{Acknowledgements}  % Uncomment (if needed) for acknowledgements
% \tableofcontents        % Uncomment (if needed) for final draft
% \listoffigures        % Uncomment (if needed) for final draft
% \listoftables         % Uncomment (if needed) for final draft

\chapter{Introduction}
This technical note is aimed to document the design of ideal cross domain bulk
transfer mechanism that we wish to have in Barrelfish.


\section{Existing implementation in Barrelfish}
There are two bulk transfer mechanisms that exist in the current(as of August
2011) implementation of Barrelfish.
\texttt{$lib/barrelfish/bulk\_transfer.c$}
and \textit{pbufs} used in the network stack.  We will discuss them briefly in
following two sub-sections.

\subsection{$bulk\_transfer.c/h$}
This is the formal bulk transfer method supported by Barrelfish.  In brief, this 
facility works by sharing a continuous piece of memory (in form of capability) 
between two domains.  These domains then map this physical memory into their 
virtual address-space.  The virtual address where this shared memory is loaded
can be different.  So, participating processes can't exchange the virtual 
addresses directly.  Bulk transfer mechanism works by dividing the entire 
memory area into the blocks of fixed size.  The domain which initiated the 
bulk transfer is responsible for managing these blocks.  This responsibility
includes following.
\begin{enumerate}
  \item Allocation and deallocation of the blocks.
  \item Maintaining the information about free and used blocks.
\end{enumerate}
So, this bulk transfer facility works in master-slave setup.  The master
allocates the blocks and passes the index of the block to slave.  
Once the block is passed to the slave, master should not touch it.  
The slave can locate the block by adding the \texttt{(index*block-size)} to the
virtual address pointing to the beginning of the shared memory area.
Now slave can read/modify the contents in the memory area of this block.  Once
the slave is done with accessing the block, it can pass the block-index
back to the master.  Master can either access the block or release it,
so that it will be added to the list of free blocks.

\subsubsection{Limitations}
\paragraph{Security}
This transfer mechanism can only be used between domains which are
co-operative and willing to follow the protocol set for using the bulk
transfer mechanism.  Malicious domain can corrupt the data in shared memory
or can refuse to return the blocks once passed to it.

\paragraph{More than two domains}
The design of this bulk transport does not stop you from using it with
more than two domains as long as the protocol is followed.  But current
implementation does not track which domain is holding the buffer. So,
one can use the current implementation with multiple domains as long
as the domains are co-operative and applications are willing to deal
with added complexity of tracking which domains hold which buffers.

\paragraph{Use}
Due to the relatively simple nature of the implementation, its use is
limited only to few places. And hence it is not throughly tested in
various scenarios.  

\subsection{pbufs}
The network stack uses its own custom bulk transport mechanisms.  Lets
call them pbufs.  These pbufs work in similar way to above mechanism,
but are more flexible.  The application shares some piece of physical memory
with network driver which is used as shared memory.  Then application
creates a list of pbuf structures, each one of them holding an offset
within shared memory, length, \textit{pbuf-id} and the shared memory id to
which they belong.  The key difference here is that the these pbufs 
may not hold consecutive memory locations even when \textit{pbuf-id}'s are
consecutive.  In contrast with Barrelfish bulk-transfer where buffer-id
value is enough to find the location of buffer within shared memory,
pbuf needs to store the offset separately in another list.  So,
pbufs just provide another layer of indirection to allow more
flexible use of memory.


The way this mechanism currently works is that, application creates
pool of initial pbufs and registers them with network driver.  Both,
application and network driver maintain the list of pbufs in their
private memory.  This list is kept in synchronization by sending
explicit messages.  Now each pbuf can in principle point to any
buffer of any size, located anywhere in the shared memory.


This flexibility is used by application when it receives a data from
the network driver.  Application creates a new pbuf structure with
same pbuf-id but pointing at new buffer location and send it back to
the driver as new free pbuf to use.  And the location of previous
buffer is used for processing the data.  This way, application can
return the pbufs back to driver ASAP without getting affected by how
long does the data processing takes.  When application is done with
processing the data in that buffer, it releases that buffer.  This
released buffer is then used to create new pbuf which will be
registered again with driver in future. 

\subsubsection{Limitations}

\paragraph{Needs more memory}
The shared memory needs to be bigger than the memory shared with
network driver in form of registered pbufs.  This is because, at any
given point in time, some pbufs will be in application data processing
phase and hence can-not be used by driver to receive the new data.

\paragraph{Data corruption}
This bulk-transfer mechanism assumes co-operative domains.  Both
domains have read/write access to the shared memory at all the time.
This means that if they do not follow the protocol correctly, they may
end up writing at same physical location, leading to data corruption.

\paragraph{Complicated memory reclamation}
The current implementation of pbufs assumes that application can have
multiple threads and all of them can access the data in pbufs which
are delivered to the application.  This complicates the problem of
detecting when exactly all threads are done with accessing the
particular pbuf.  The current implementation uses a reference count
mechanism for this detection.  Even though this memory reclamation is
functional, it is complicated and one can easily get it wrong, leading
to memory leaks.

\paragraph{Supporting more than two domains}
In theory, this design can be used with more than two domains which
are co-operating with each other, but implementation is not designed
with such a case in mind.  The problematic issues will be tracking
which memory buffer is with which domain, and when the memory buffer
can be reused.

\subsubsection{More issues with current network stack implementation}
Following issues are not exactly due to the bulk transfer mechanisms,
but are mainly the implementation issues of network stack.  I am
documenting them here for sake of completeness.  Also, knowing the
issues in current stack will help in understanding the design
decisions made.

\paragraph{Favors the setup with driver on different core}
It is optimized for the case where application and network driver are
running on separate core.  It is reasonable to prefer such scenarios
for multicore architectures where presence of large number of cores is
assumed.  This preference has resulted on overly dependence on
efficiency of UMP messages.  Current implementation sends around 2
messages (send-packet, tx-done) for transmitting single packet
assuming that packet fits into one pbuf.  Similarly on packet
receiving side, each received packet involves 2 messages 
(register-pbuf, packet-received).  The performance impact of these
messages is not strongly visible when application and driver are on
different core.  But there are cases like SCC where it is good idea 
to run the driver on same core as an application.  In such scenarios
LMP's are used for communication which involves doing context-switch in
sending every message. This leads to lot of performance
in-efficiencies when current network stack implementation is used in
LMP contexts.  Specially on those hardware where cost of
context-switch is relatively high.


I have tried to measure the performance difference on \texttt{nos4-6}
boxes with webserver cache loading test, but the performance
difference was not significant.  I have not investigated the reasons
behind them yet, but my preliminary guess was that context switches on
these machines are quite efficient or webserver cache loading test
does not stress the network stack enough to see any significant
difference.


\section{Questions to be asked about bulk transfer mechanisms}
\begin{enumerate}
  \item Does it uses memory efficiently?
  \item Can it be used with mutually non-trusting domains?
  \item Can it be used across more than two domains?
  \item Can consumer/producer put a back-pressure on other side?
  \item Can it support multiple producer and multiple consumers?
  \item Can it allow implementation of \textit{True Zero Copy}
  sending and receiving?
  \item Can it lead to deadlock if one domain is only sending without
  checking for arriving messages?
\end{enumerate}

\section{Desired properties from bulk transfer mechanisms}

\section{Related work}
The issues of bulk-transport has been investigated quite a few times
in the past, so the solution of this problem is known to the large
extent.  What this document is aiming is to choose right approach and
right solution which will suit the needs of the Barrelfish.


The work which has particularly influenced this design are
\begin{enumerate}
  \item F-bufs: Helped in clearly understanding the problems in
  designing cross-domain transfer facility.
  \item R-bufs: Helped in understanding how to implement
  scatter-gather packet sending and receiving.
  \item Beltway buffers: Gave clear idea of how to design a
  ring-buffer style interface which can be easily interfaced with
  hardware. 
\end{enumerate}

\section{Requirements of Barrelfish}

\begin{enumerate}
  \item Avoid data copy as much as possible, if can't avoid,
  push it into user-space/user-core.
  \item Ability to batch the notifications.
  \item True zero copy capability(scatter-gather packet
  sending/receiving).
  \item Should work with more than two domains.
  \item Should work with multiple producers and multiple consumers.
\end{enumerate}

\chapter{Design}
This chapter discusses the design of bulk-transport mechanism and
protocol that will be implemented in future for Barrelfish network
stack.


\section{Bird's eye view of design}
We are aiming to design a cross domain bulk transport mechanism,
which supports following features.
\begin{enumerate}
  \item Should reduce the data copy as much as possible.
  \item Should exploit the fact that complete isolation of data
  is not always needed.
  \item More than two separate domains should be able to share
  the data without copying it.
  \item Number of explicit notifications needed should be low.
  \item It should work in \textit{single producer, single consumer}
  and \textit{single producer, multiple consumers}
\end{enumerate}

\section{Terminology used}
This section briefly explains the term used in the design 
description.  This should help in reducing the ambiguity of the design.
\begin{enumerate}
  \item \textbf{Generator:} An entity which is generating the data.  It
  can be software or hardware entity.  For example, Network
  Interface Card(NIC) is a generator, or the application code
  which is generating the packets to send over the NIC.
  \item \textbf{data-element:} The data-element is what generator 
  is generating. In case of NIC device, a packet will be a
  data-element.
  \item \textbf{Producer:} An entity which is managing the generated data.
  This management contains providing access mitigation, notifying
  interested consumers, reclaiming the memory when data-element
  expires.  The example of producer will be \textit{NIC device
  driver}.  


  The distinction between generator and producer is
  rather fine.  They are intentionally kept separate because
  generators can have hardware constraints which does not allow
  it to do all functionalities.
  \item \textbf{Consumer:} An entity which is consuming the data-elements. 
  \item \textbf{Slot:} The contiguous piece of memory where data-element is
  entirely or partially stored. A data-element can span over
  more than one slots. 
  \item \textbf{Shared-memory:} A contiguous piece of memory accessible 
   from producer in read/write mode and consumers in read only mode.
  \item \textbf{Free slot:} A slot which is available, and can be given
  to generator.
  \item \textbf{In-generation slot:} A slot is given to generator. Only 
  the generator should read/write this slot now.  This is the only
  mode when data can be written in the slot.
  \item \textbf{In-consumption slot:} One or more consumers are currently
  consuming this slot.  In this mode, slot is strictly read-only
  no one should modify the data in slot in this mode.
  \item  
\end{enumerate}

\section{The Producer}
This section describes the producer and generator together with
their responsibilities.  Even though producer and generator can
be a different entities internally, the consumers will only see
the producer interface.

\subsection{Generator}
Lets make a rough sketch of typical generator.  Here we are aiming
for the NIC devices.  Typically, these devices will have an
internal queue (RX-queue) of slots where the packet received
from wire will be copied.  These generators are also capable
of generating a notification in form of interrupt.  Now the
way RX-queues work on different devices differ in some way.
Few devices expect contiguous memory in the slots whereas
other devices are capable of DMAing the packet in non-contiguous
memory as well.

	
The software generators like application
logic which creates new packet to send out is much simpler than
NIC devices but can have quirky behavior like above described
NIC devices.


\subsection{The Producer abstraction}
The producer abstraction is what consumer sees and interacts with.
The producer creates a 
\textbf{shared-memory} area which is a contiguous piece of memory
which is writable only by producer and can be mapped as read-only
memory by consumers.  The virtual address where the shared-memory
is mapped need not be same for the producer and consumers.

\subsection{Shared-memory}
Shared memory is the area where producer will generate the data
and consumers will read it from. The producer breaks this
shared memory area into \textit{slots}.  The size of slot
is based on the capability of generator.  If generator needs
continuous memory to produce data-element then the size of
slot will be the size of biggest data-element.  If the
generator can DMA the data-element into multiple non-contiguous
memory locations then, the slot size should be based on
the average data-element size.  In any case, the slot size
must alway be a multiple of cache size and every slot should
be cache aligned.  This is mostly due to the fact that each
data element might get consumed by different consumer running
on different cores, and we do not want cache conflicts when
they are accessing different data-elements.


The dis-advantage of having slots of size of largest data-element
is that it leads to internal fragmentation and waste of memory 
when packets are small.  On other hand, when slot size is small,
we reduce the capacity of NIC hardware as typically RX-queue
will have limited number of entries, and by using small sized
slots, we will fill up the entries quickly.


\paragraph{Ideal size of shared memory}
Ideally, shared-memory should be big enough to accommodate the queue
of the generator (ie. All the descriptors in RX-queue of NIC device)
and all the data-elements which are with consumers and are not released 
back (in-consumption state).  So, the total memory size should be
proportional to the capacity of the generator(ie. NIC device) and 
number of consumers.  As each consumer is allowed to keep only fixed 
amount of in-consumption packets, slow consumers will not adversely 
affect the fast consumers.


This has other implications like, the number of possible applications
should be known in advance for optimal use of memory.

\subsubsection{A meta-slot structure}
This structure hold the additional information about each slot in
shared-memory.  It can be seen as index table on shared-memory.
This structure is private to Producer and is used to manage the
slots.  Following are the key elements of this structure.
\begin{enumerate} 
  \item \textbf{slot-id:} The slot identifier. This should help
  in calculating the offset of slot within shared-memory.
  \item \textbf{state:} The state of slot (Free, in-Generation, 
  in-Consumption)
  \item \textbf{data-len:} Size of valid data in the slot.
  \item \textbf{Counter:} How many consumers are accessing it.
  \item \textbf{Next, prev:} Used to maintain the list (eg. Free slots)
\end{enumerate} 

\paragraph{Producer initialization}
At producer initialization, following steps will happen.
\begin{enumerate} 
  \item Shared-memory will be divided into slots.
  \item A meta-slot list will be created and initialized in free state.
  \item Add slots in generator queue (RX-queue for NIC) and change.
  \item Create an empty list of consumers.
\end{enumerate} 

\paragraph{Possible events received by producer}
\begin{enumerate} 
  \item Data-element generated (from generator).
  \item generator queue full (from generator).
  \item Data-element consumed.
  \item Increment data-element counter.
  \item New consumer registered.
  \item Consumer de-registered.
  \item Consumer registers up-call for new data.
\end{enumerate} 

\paragraph{Possible events generated by producer}
\begin{enumerate} 
  \item add empty slot (to generator). 
  \item New data element.
  \item Consumer queue full. 
\end{enumerate} 

\section{Consumer}
This section describes the internals of the consumer.

\subsection{Consumer registration}
As part of initialization process, consumer should register itself
with producer.  This initialization process includes following steps.
\begin{enumerate} 
  \item Map the shared-memory block as read-only in the virtual
  address space.
  \item Create a contiguous memory slot to maintain a consumer-queue.
  \item Initialize the this memory slot "appropriately".
  \item Share this memory slot with producer.
\end{enumerate} 


\subsection{Consumer-queue}
This section describes the consumer queue in details.

\paragraph{Possible events received by consumer}
This paragraph describes the event received by consumers and how
to react to them.

\paragraph{Possible events generated by consumer}
This paragraph describes the events generated by consumers.

\section{Notification mechanism}
Describe the notification mechanism.

\section{Memory management mechanism}
Describe the memory management mechanism

\section{How sharing will happen}
Details of how things will be managed when there are more than two
applications. Can more than two applications work in sync with true 
zero copy?

\section{Overview}
\end{document}
